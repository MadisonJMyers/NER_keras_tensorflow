{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmyers/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from clr_callback import *\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input\n",
    "# from tensorflow.python.keras.optimizers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Embedding, Dropout, Bidirectional, LSTM, Concatenate, Reshape, Lambda, Input, Activation, Masking\n",
    "from tensorflow.python.keras.layers import concatenate\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.optimizers import Adam, SGD\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import one_hot\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras_contrib.layers import CRF\n",
    "from model.data_utils import get_trimmed_glove_vectors, load_vocab, get_processing_word, CoNLLDataset, get_trimmed_glove_vectors, load_vocab, get_processing_word, minibatches, get_chunks, pad_sequences\n",
    "from model.ner_model import NERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download new data\n",
    "#source: https://github.com/synalp/NER\n",
    "train_filename = \"data/coNLL/eng/eng.train.iob\"\n",
    "dev_filename = \"data/coNLL/eng/eng.testa.iob\"\n",
    "test_filename = \"data/coNLL/eng/eng.testb.iob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_chars = True\n",
    "max_iter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$NUM$': 13,\n",
       " '$UNK$': 9,\n",
       " '.': 16,\n",
       " 'a': 11,\n",
       " 'actor': 2,\n",
       " 'american': 4,\n",
       " 'an': 8,\n",
       " 'and': 0,\n",
       " 'economic': 12,\n",
       " 'european': 22,\n",
       " 'french': 7,\n",
       " 'in': 5,\n",
       " 'is': 1,\n",
       " 'jean': 10,\n",
       " 'lives': 21,\n",
       " 'new': 19,\n",
       " 'oscar': 20,\n",
       " 'pierre': 14,\n",
       " 'political': 17,\n",
       " 'the': 3,\n",
       " 'union': 6,\n",
       " 'won': 18,\n",
       " 'york': 15}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words = load_vocab(\"data/words.txt\")\n",
    "vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 23,\n",
       " 'A': 5,\n",
       " 'E': 17,\n",
       " 'F': 9,\n",
       " 'J': 18,\n",
       " 'N': 8,\n",
       " 'P': 25,\n",
       " 'T': 20,\n",
       " 'U': 21,\n",
       " 'Y': 3,\n",
       " 'a': 15,\n",
       " 'c': 27,\n",
       " 'd': 11,\n",
       " 'e': 7,\n",
       " 'h': 24,\n",
       " 'i': 10,\n",
       " 'k': 16,\n",
       " 'l': 1,\n",
       " 'm': 26,\n",
       " 'n': 12,\n",
       " 'o': 14,\n",
       " 'p': 22,\n",
       " 'r': 4,\n",
       " 's': 13,\n",
       " 't': 6,\n",
       " 'u': 19,\n",
       " 'v': 2,\n",
       " 'w': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_chars = load_vocab(\"data/chars.txt\")\n",
    "vocab_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 7,\n",
       " 'B-MISC': 3,\n",
       " 'B-ORG': 5,\n",
       " 'B-PER': 1,\n",
       " 'I-LOC': 8,\n",
       " 'I-MISC': 4,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 2,\n",
       " 'O': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: this order could be arbitrary, with values in the interval [0, num_tags]\n",
    "# ALSO: there should be a difference between a null tag, and a padded label\n",
    "vocab_tags = load_vocab(\"data/tags.txt\")\n",
    "vocab_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_char = len(vocab_chars)\n",
    "n_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tags = (len(vocab_tags)+1) #+1 if different vocab_tags\n",
    "n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coNLL data for validation\n",
    "dev = CoNLLDataset(dev_filename, get_processing_word(vocab_words, vocab_chars,lowercase=True, chars=use_chars),\n",
    "                  get_processing_word(vocab_tags, lowercase=False, allow_unk=False), max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coNLL data for train\n",
    "train = CoNLLDataset(train_filename, get_processing_word(vocab_words, vocab_chars,lowercase=True, chars=use_chars),\n",
    "                  get_processing_word(vocab_tags, lowercase=False, allow_unk=False), max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coNLL data for test\n",
    "test = CoNLLDataset(test_filename, get_processing_word(vocab_words, vocab_chars,lowercase=True, chars=use_chars),\n",
    "                  get_processing_word(vocab_tags, lowercase=False, allow_unk=False), max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_vocab(filename):\n",
    "    \"\"\"Load vocab from file\n",
    "    Args:\n",
    "        filename: path to the glove vectors\n",
    "    Returns:\n",
    "        vocab: set() of strings\n",
    "    \"\"\"\n",
    "    print(\"Building vocab...\")\n",
    "    vocab = set()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            word = line.strip().split(' ')[0]\n",
    "            vocab.add(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_data = np.load(\"data/glove.6B.300d.trimmed.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = emb_data[\"embeddings\"]\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_word = 300 #End to end paper uses 30\n",
    "dim_char = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_char = 100 # lstm on chars\n",
    "hidden_size_lstm = 300 # lstm on word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 75 #End to end paper saw best results at 50 epochs\n",
    "lr = 0.0105 #0.001 #End to end uses learning rate of 0.01 for POS tagging and 0.015 for NER where lr is updated on each epoch with decay rate 0.05\n",
    "lr_decay = 0.0005 #lr/nepochs #0.05 #GG uses 0.9; paper uses 0.05\n",
    "batch_size = 10 #20 #End to end paper uses 10 #eval at 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make use of minibatches with fit_generator\n",
    "# for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "words, labels = list(minibatches(train, len(train)))[0]  # NOTE: len(train) will return entire dataset!\n",
    "#GG's version\n",
    "char_ids, word_ids = zip(*words)\n",
    "word_ids, sequence_lengths = pad_sequences(word_ids, pad_tok=9) #word_ids = vocab_chars?\n",
    "char_ids, word_lengths = pad_sequences(char_ids, pad_tok=9, nlevels=2)\n",
    "labels, _ = pad_sequences(labels, pad_tok=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation/dev\n",
    "words_dev, labels_dev = list(minibatches(dev, len(dev)))[0]  \n",
    "char_ids_dev, word_ids_dev = zip(*words_dev)\n",
    "word_ids_dev, sequence_lengths_dev = pad_sequences(word_ids_dev, pad_tok=9)\n",
    "char_ids_dev, word_lengths_dev = pad_sequences(char_ids_dev, pad_tok=9, nlevels=2)\n",
    "labels_dev, _ = pad_sequences(labels_dev, pad_tok=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "words_test, labels_test = list(minibatches(test, len(test)))[0]  \n",
    "char_ids_test, word_ids_test = zip(*words_test)\n",
    "word_ids_test, sequence_lengths_test = pad_sequences(word_ids_test, pad_tok=9)\n",
    "char_ids_test, word_lengths_test = pad_sequences(char_ids_test, pad_tok=9, nlevels=2)\n",
    "labels_test, _ = pad_sequences(labels_test, pad_tok=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5 # needs to be set before Dropout function- GG 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_input = Input((None,))\n",
    "mask_word = Masking(mask_value=9)(word_emb_input)\n",
    "word_emb_output = Embedding(n_words, dim_word, weights=[embeddings], trainable=False)(mask_word)\n",
    "# word_emb_output = Dropout(dropout)(word_emb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#end to end paper claims to have applied dropout layer on character embeddings before inputting to a CNN in addition to before both layers of BLSTM\n",
    "# char_emb_input = Input((max_seq_length, max_word_length)) \n",
    "char_emb_input = Input((None, None))\n",
    "#comes in as sentences, words, characters and for the character part we want to just operate it over the character sentence by number of words and seq of characters so reshape so we have words by characters\n",
    "char_emb_output = Lambda(lambda x: tf.keras.backend.reshape(x, (-1, tf.keras.backend.shape(x)[-1])))(char_emb_input)\n",
    "mask_char = Masking(mask_value=9)(char_emb_output)  # TODO: make -1 a variable\n",
    "char_emb_output = Embedding(n_char, dim_char)(mask_char) #need weights here?\n",
    "# 2 sided LSTM below that we can change with forward and backward to see which is better performing\n",
    "# char_emb_output = Bidirectional(LSTM(hidden_size_char, return_sequences=False))(char_emb_output)\n",
    "char_emb_output = Dropout(dropout)(char_emb_output)\n",
    "fw_LSTM = LSTM(hidden_size_char, return_sequences=False)(char_emb_output) #is this right?\n",
    "bw_LSTM = LSTM(hidden_size_char, return_sequences=False, go_backwards=True)(char_emb_output)\n",
    "char_emb_output = concatenate([fw_LSTM, bw_LSTM])\n",
    "char_emb_output = Dropout(dropout)(char_emb_output)\n",
    "char_emb_output = Lambda(lambda x, z: tf.keras.backend.reshape(x, (-1, tf.shape(z)[1], 2 * hidden_size_char)), arguments={\"z\": word_emb_input})(char_emb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenates word embedding and character embedding\n",
    "x = concatenate([word_emb_output, char_emb_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(dropout)(x)\n",
    "x = Bidirectional(LSTM(hidden_size_lstm, return_sequences=True))(x)  #should we turn this into two layers (fw and bw)?\n",
    "# fw_LSTM_2 = LSTM(hidden_size_lstm, return_sequences=True)(x) #is this right?\n",
    "# bw_LSTM_2 = LSTM(hidden_size_lstm, return_sequences=True, go_backwards=True)(x)\n",
    "# x = concatenate([fw_LSTM_2, bw_LSTM_2])\n",
    "x = Dropout(dropout)(x)\n",
    "# scores = Dense(n_tags, activity_regularizer=regularizers.l1(0.001))(x) \n",
    "scores = Dense(n_tags)(x) \n",
    "softmax = Activation(\"softmax\")(scores)\n",
    "crf_layer = CRF(n_tags)\n",
    "# crf = crf_layer(scores) #should we add this to attach to the softmax model? with SGD and gradiet clipping of 5.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_softmax = Model([word_emb_input, char_emb_input], softmax) #should these be input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf = Model([word_emb_input, char_emb_input], crf) #should these be input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None)         0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    2800        masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 100)    0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100)          80400       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100)          80400       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    6900        masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 200)    0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 500)    0           embedding_1[0][0]                \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 500)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 600)    1922400     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, 600)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 10)     6010        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 10)     0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,098,910\n",
      "Trainable params: 2,092,010\n",
      "Non-trainable params: 6,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_softmax.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_op = Adam(lr=lr, decay=lr_decay)\n",
    "# sgd = SGD(lr=lr, momentum=momentum, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_softmax.compile(optimizer=adam_op, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.compile(loss=crf_layer.loss_function, optimizer=adam_op, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "char_ids_arr = np.array(char_ids)\n",
    "word_ids_arr = np.array(word_ids)\n",
    "labels_arr = np.array(labels)\n",
    "labels_arr_one_hot = np.eye(10)[labels] #10 if vocab_tags are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev/validation\n",
    "char_ids_arr_dev = np.array(char_ids_dev)\n",
    "word_ids_arr_dev = np.array(word_ids_dev)\n",
    "labels_arr_dev = np.array(labels_dev)\n",
    "labels_arr_one_hot_dev = np.eye(10)[labels_dev] #10 if vocab_tags are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "char_ids_arr_test = np.array(char_ids_test)\n",
    "word_ids_arr_test = np.array(word_ids_test)\n",
    "labels_arr_test = np.array(labels_test)\n",
    "labels_arr_one_hot_test = np.eye(10)[labels_test] #10 if vocab_tags are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = datetime.strftime(datetime.today(), \"%y%m%d_%H%M%S\")\n",
    "# base_dir = f\"models/{date}\"\n",
    "# if not os.path.exists(base_dir):\n",
    "#     os.makedirs(base_dir)\n",
    "# model_checkpoint = tf.keras.callbacks.ModelCheckpoint(base_dir + \"/{val_loss}_{epoch:03d}.hdf5\")\n",
    "# tb_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "# callbacks = [model_checkpoint, tb_callback]\n",
    "# callbacks = [clr]\n",
    "# print(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14041 samples, validate on 3250 samples\n",
      "Epoch 1/75\n",
      "14041/14041 [==============================] - 842s 60ms/step - loss: 0.0844 - acc: 0.9743 - val_loss: 0.0641 - val_acc: 0.9801\n",
      "Epoch 2/75\n",
      "14041/14041 [==============================] - 837s 60ms/step - loss: 0.0621 - acc: 0.9803 - val_loss: 0.0570 - val_acc: 0.9818\n",
      "Epoch 3/75\n",
      "14041/14041 [==============================] - 837s 60ms/step - loss: 0.0577 - acc: 0.9816 - val_loss: 0.0542 - val_acc: 0.9830\n",
      "Epoch 4/75\n",
      "14041/14041 [==============================] - 837s 60ms/step - loss: 0.0538 - acc: 0.9828 - val_loss: 0.0512 - val_acc: 0.9840\n",
      "Epoch 5/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0513 - acc: 0.9836 - val_loss: 0.0478 - val_acc: 0.9849\n",
      "Epoch 6/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0488 - acc: 0.9843 - val_loss: 0.0458 - val_acc: 0.9855\n",
      "Epoch 7/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0469 - acc: 0.9849 - val_loss: 0.0443 - val_acc: 0.9861\n",
      "Epoch 8/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0455 - acc: 0.9854 - val_loss: 0.0423 - val_acc: 0.9866\n",
      "Epoch 9/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0441 - acc: 0.9858 - val_loss: 0.0408 - val_acc: 0.9870\n",
      "Epoch 10/75\n",
      "14041/14041 [==============================] - 837s 60ms/step - loss: 0.0429 - acc: 0.9861 - val_loss: 0.0401 - val_acc: 0.9872\n",
      "Epoch 11/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0416 - acc: 0.9865 - val_loss: 0.0393 - val_acc: 0.9874\n",
      "Epoch 12/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0407 - acc: 0.9868 - val_loss: 0.0385 - val_acc: 0.9879\n",
      "Epoch 13/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0397 - acc: 0.9872 - val_loss: 0.0376 - val_acc: 0.9881\n",
      "Epoch 14/75\n",
      "14041/14041 [==============================] - 835s 60ms/step - loss: 0.0389 - acc: 0.9874 - val_loss: 0.0369 - val_acc: 0.9883\n",
      "Epoch 15/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0383 - acc: 0.9876 - val_loss: 0.0365 - val_acc: 0.9886\n",
      "Epoch 16/75\n",
      "14041/14041 [==============================] - 835s 60ms/step - loss: 0.0377 - acc: 0.9878 - val_loss: 0.0362 - val_acc: 0.9885\n",
      "Epoch 17/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0369 - acc: 0.9880 - val_loss: 0.0354 - val_acc: 0.9887\n",
      "Epoch 18/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0364 - acc: 0.9881 - val_loss: 0.0351 - val_acc: 0.9889\n",
      "Epoch 19/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0359 - acc: 0.9883 - val_loss: 0.0345 - val_acc: 0.9891\n",
      "Epoch 20/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0353 - acc: 0.9885 - val_loss: 0.0342 - val_acc: 0.9891\n",
      "Epoch 21/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0351 - acc: 0.9886 - val_loss: 0.0339 - val_acc: 0.9893\n",
      "Epoch 22/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0347 - acc: 0.9887 - val_loss: 0.0338 - val_acc: 0.9894\n",
      "Epoch 23/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0343 - acc: 0.9889 - val_loss: 0.0335 - val_acc: 0.9896\n",
      "Epoch 24/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0334 - acc: 0.9891 - val_loss: 0.0331 - val_acc: 0.9896\n",
      "Epoch 25/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0333 - acc: 0.9891 - val_loss: 0.0328 - val_acc: 0.9896\n",
      "Epoch 26/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0329 - acc: 0.9894 - val_loss: 0.0327 - val_acc: 0.9897\n",
      "Epoch 27/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0327 - acc: 0.9894 - val_loss: 0.0324 - val_acc: 0.9898\n",
      "Epoch 28/75\n",
      "14041/14041 [==============================] - 834s 59ms/step - loss: 0.0324 - acc: 0.9895 - val_loss: 0.0321 - val_acc: 0.9900\n",
      "Epoch 29/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0320 - acc: 0.9896 - val_loss: 0.0318 - val_acc: 0.9901\n",
      "Epoch 30/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0318 - acc: 0.9896 - val_loss: 0.0315 - val_acc: 0.9902\n",
      "Epoch 31/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0314 - acc: 0.9897 - val_loss: 0.0316 - val_acc: 0.9902\n",
      "Epoch 32/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0313 - acc: 0.9897 - val_loss: 0.0315 - val_acc: 0.9902\n",
      "Epoch 33/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0308 - acc: 0.9899 - val_loss: 0.0312 - val_acc: 0.9903\n",
      "Epoch 34/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0305 - acc: 0.9900 - val_loss: 0.0313 - val_acc: 0.9903\n",
      "Epoch 35/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0304 - acc: 0.9901 - val_loss: 0.0309 - val_acc: 0.9903\n",
      "Epoch 36/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0301 - acc: 0.9902 - val_loss: 0.0308 - val_acc: 0.9906\n",
      "Epoch 37/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0301 - acc: 0.9902 - val_loss: 0.0307 - val_acc: 0.9904\n",
      "Epoch 38/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0296 - acc: 0.9903 - val_loss: 0.0306 - val_acc: 0.9904\n",
      "Epoch 39/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0296 - acc: 0.9903 - val_loss: 0.0304 - val_acc: 0.9905\n",
      "Epoch 40/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0295 - acc: 0.9904 - val_loss: 0.0304 - val_acc: 0.9906\n",
      "Epoch 41/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0292 - acc: 0.9904 - val_loss: 0.0303 - val_acc: 0.9906\n",
      "Epoch 42/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0291 - acc: 0.9905 - val_loss: 0.0300 - val_acc: 0.9907\n",
      "Epoch 43/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0287 - acc: 0.9906 - val_loss: 0.0301 - val_acc: 0.9907\n",
      "Epoch 44/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0287 - acc: 0.9906 - val_loss: 0.0299 - val_acc: 0.9907\n",
      "Epoch 45/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0285 - acc: 0.9907 - val_loss: 0.0301 - val_acc: 0.9907\n",
      "Epoch 46/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0301 - val_acc: 0.9908\n",
      "Epoch 47/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0281 - acc: 0.9908 - val_loss: 0.0300 - val_acc: 0.9908\n",
      "Epoch 48/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0278 - acc: 0.9909 - val_loss: 0.0298 - val_acc: 0.9908\n",
      "Epoch 49/75\n",
      " 9460/14041 [===================>..........] - ETA: 4:14 - loss: 0.0276 - acc: 0.9910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0271 - acc: 0.9911 - val_loss: 0.0294 - val_acc: 0.9910\n",
      "Epoch 53/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0272 - acc: 0.9911 - val_loss: 0.0294 - val_acc: 0.9910\n",
      "Epoch 54/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0269 - acc: 0.9912 - val_loss: 0.0293 - val_acc: 0.9910\n",
      "Epoch 55/75\n",
      "14041/14041 [==============================] - 835s 60ms/step - loss: 0.0266 - acc: 0.9912 - val_loss: 0.0291 - val_acc: 0.9912\n",
      "Epoch 56/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0264 - acc: 0.9914 - val_loss: 0.0293 - val_acc: 0.9911\n",
      "Epoch 57/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0264 - acc: 0.9914 - val_loss: 0.0290 - val_acc: 0.9912\n",
      "Epoch 58/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0263 - acc: 0.9914 - val_loss: 0.0291 - val_acc: 0.9912\n",
      "Epoch 59/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0259 - acc: 0.9915 - val_loss: 0.0288 - val_acc: 0.9914\n",
      "Epoch 60/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0258 - acc: 0.9915 - val_loss: 0.0288 - val_acc: 0.9914\n",
      "Epoch 61/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0259 - acc: 0.9915 - val_loss: 0.0287 - val_acc: 0.9914\n",
      "Epoch 62/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0257 - acc: 0.9915 - val_loss: 0.0287 - val_acc: 0.9914\n",
      "Epoch 63/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0257 - acc: 0.9915 - val_loss: 0.0285 - val_acc: 0.9914\n",
      "Epoch 64/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0255 - acc: 0.9916 - val_loss: 0.0287 - val_acc: 0.9914\n",
      "Epoch 65/75\n",
      "14041/14041 [==============================] - 834s 59ms/step - loss: 0.0253 - acc: 0.9916 - val_loss: 0.0287 - val_acc: 0.9914\n",
      "Epoch 66/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0251 - acc: 0.9917 - val_loss: 0.0287 - val_acc: 0.9914\n",
      "Epoch 67/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0250 - acc: 0.9918 - val_loss: 0.0285 - val_acc: 0.9915\n",
      "Epoch 68/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0247 - acc: 0.9919 - val_loss: 0.0286 - val_acc: 0.9915\n",
      "Epoch 69/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0248 - acc: 0.9918 - val_loss: 0.0284 - val_acc: 0.9915\n",
      "Epoch 70/75\n",
      "14041/14041 [==============================] - 834s 59ms/step - loss: 0.0245 - acc: 0.9920 - val_loss: 0.0285 - val_acc: 0.9916\n",
      "Epoch 71/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0246 - acc: 0.9919 - val_loss: 0.0282 - val_acc: 0.9915\n",
      "Epoch 72/75\n",
      "14041/14041 [==============================] - 836s 60ms/step - loss: 0.0246 - acc: 0.9919 - val_loss: 0.0282 - val_acc: 0.9915\n",
      "Epoch 73/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0241 - acc: 0.9921 - val_loss: 0.0282 - val_acc: 0.9916\n",
      "Epoch 74/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0243 - acc: 0.9920 - val_loss: 0.0281 - val_acc: 0.9916\n",
      "Epoch 75/75\n",
      "14041/14041 [==============================] - 835s 59ms/step - loss: 0.0237 - acc: 0.9922 - val_loss: 0.0283 - val_acc: 0.9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7ff89c0290f0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add callbacks:\n",
    "# early stopping and saving best parameters\n",
    "# learning rate decay\n",
    "# tensorboard\n",
    "# number of epochs without improving is 0 (for early stopping)\n",
    "# could add gradient clipping (optional)\n",
    "model_softmax.fit([word_ids_arr, char_ids_arr], labels_arr_one_hot, batch_size=batch_size, epochs=nepochs, validation_data=([word_ids_arr_dev, char_ids_arr_dev], labels_arr_one_hot_dev)) # validation_split=0.3\n",
    "#fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "#fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "\n",
    "# model_softmax.save(f\"{base_dir}/train_softmax.hdf5\") #final_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_softmax.save_weights(\"softmax_test_5_31_1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.fit([word_ids_arr, char_ids_arr], labels_arr_one_hot, batch_size=batch_size, epochs=nepochs, validation_split=0.33)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_crf.save(\"crf.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.save(\"crf_with_val.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir  #models/180222_215523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_softmax.load_weights(\"softmax_test_5_31_1.hdf5\")\n",
    "# model_softmax.load_weights(f\"{base_dir}/train_softmax.hdf5\")#\"models/180222_215523/final_softmax.hdf5\")#\"0.11342436582348703_050.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_softmax = model_softmax.predict([word_ids_arr, char_ids_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.load_weights(\"crf_with_val.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_crf = model_crf.predict([word_ids_arr, char_ids_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prediction_crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "# scores_crf = model_crf.evaluate([word_ids_arr, char_ids_arr], labels_arr_one_hot) #x_test, y_test (when testing)\n",
    "# print(\"%s: %.2f%%\" % (model_crf.metrics_names[1], scores_crf[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_softmax = model_softmax.evaluate([word_ids_arr, char_ids_arr], labels_arr_one_hot) #x_test, y_test (when testing)\n",
    "# print(\"%s: %.2f%%\" % (model_softmax.metrics_names[1], scores_softmax[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train F1 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate Training##\n",
    "def extract_data(dataset):\n",
    "    \"\"\"Extract words and labels from a dataset.\n",
    "    \n",
    "    Args:\n",
    "      dataset: A CoNLL dataset.\n",
    "    \n",
    "    Returns:\n",
    "      Word ids, char ids, and labels, from a CoNLL dataset,\n",
    "      all as NumPy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    words, labels = list(minibatches(dataset, len(dataset)))[0]  # NOTE: len(train) will return entire dataset!\n",
    "    char_ids, word_ids = zip(*words)\n",
    "    \n",
    "    word_ids, sequence_lengths = pad_sequences(word_ids, pad_tok=9)\n",
    "    char_ids, word_lengths = pad_sequences(char_ids, pad_tok=9, nlevels=2)\n",
    "    labels, _ = pad_sequences(labels, pad_tok=9)\n",
    "\n",
    "    word_ids_arr = np.array(word_ids)\n",
    "    char_ids_arr = np.array(char_ids)\n",
    "    labels_arr = np.array(labels)\n",
    "    # TODO: add one-hot encoding of labels\n",
    "    seq_lens_arr = np.array(sequence_lengths)\n",
    "    return word_ids_arr, char_ids_arr, labels_arr, seq_lens_arr\n",
    "\n",
    "\n",
    "def predict_labels(model, word_ids_arr, char_ids_arr, seq_lens_arr, batch_size=32):\n",
    "    \"\"\"Predict labels for a set of words.\n",
    "    \n",
    "    Args:\n",
    "      model: A Keras Model that accepts char ids and word ids\n",
    "        and returns label probs.\n",
    "      word_ids_arr: A NumPy array of word ids for sentences of shape\n",
    "        (num sentences, max num words).\n",
    "      char_ids_arr: A NumPy array of char ids for sentences of shape\n",
    "        (num sentences, max num words, max num chars).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words). \n",
    "    \n",
    "    Returns:\n",
    "      A NumPy array of shape (num sentences, num words)\n",
    "      containing the predicted tags for each word.\n",
    "    \"\"\"\n",
    "#     model.load_weights(\"softmax_with_masking_nine.hdf5\")\n",
    "    labels_prob_arr = model.predict([word_ids_arr, char_ids_arr], batch_size) #shape(num sentences, max num words, num tags)\n",
    "#     labels_prob_arr = model.predict(word_ids_arr, batch_size) #shape(num sentences, max num words, num tags) #DELETE\n",
    "    labels_pred_arr = np.argmax(labels_prob_arr, -1) \n",
    "    return labels_pred_arr\n",
    "\n",
    "\n",
    "def compute_metrics(labels_arr, labels_pred_arr, seq_lens_arr, vocab_tags): #commented out to play with it below but this is the og\n",
    "    \"\"\"Compute accuracy and F1.\n",
    "    \n",
    "    Args:\n",
    "      labels_arr: A NumPy array of correct tags of shape\n",
    "        (num sentences, max num words).\n",
    "      labels_pred_arr: A NumPy array of predicted tags of\n",
    "        shape (num sentences, max num words).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words).\n",
    "      vocab_tags: Dictionary of tag strings to tag numbers.\n",
    "      \n",
    "    Returns:\n",
    "      Dictionary with accuracy `acc` and F1 score `f1`.\n",
    "    \"\"\"\n",
    "    accs = []\n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "\n",
    "    for lab, lab_pred, seq_len in zip(labels_arr, labels_pred_arr, seq_lens_arr):\n",
    "        # NOTE: labels & predictions are padded to the maximum number of words\n",
    "        # in the batch.  Here, we use the actual sentence lengths to select out\n",
    "        # the actual labels and corresponding predictions.\n",
    "        lab = lab[:seq_len]\n",
    "        lab_pred = lab_pred[:seq_len]\n",
    "        for n, i in enumerate(lab_pred):\n",
    "            if i == 9:\n",
    "                lab_pred[n] = 0\n",
    "        \n",
    "        accs += [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "        \n",
    "        lab_chunks      = set(get_chunks(lab, vocab_tags))\n",
    "        lab_pred_chunks = set(get_chunks(lab_pred, vocab_tags))\n",
    "\n",
    "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "        total_preds   += len(lab_pred_chunks)\n",
    "        total_correct += len(lab_chunks)\n",
    "\n",
    "        \n",
    "    p   = correct_preds / total_preds if total_preds > 0 else 0 \n",
    "    r   = correct_preds / total_correct if total_correct > 0 else 0\n",
    "    f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "    acc = np.mean(accs)\n",
    "\n",
    "    print ({\"precision\": p})\n",
    "    print ({\"recall\": r})\n",
    "    print ({\"total_correct\": total_correct})\n",
    "    return {\"acc\": 100*acc, \"f1\": 100*f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.8365480907388863}\n",
      "{'recall': 0.8176092599685093}\n",
      "{'total_correct': 23499.0}\n",
      "{'acc': 97.19920833312871, 'f1': 82.69702578229243}\n"
     ]
    }
   ],
   "source": [
    "word_ids_arr, char_ids_arr, labels_arr, seq_lens_arr = extract_data(train) \n",
    "labels_pred_arr = predict_labels(model_softmax, word_ids_arr, char_ids_arr, seq_lens_arr)\n",
    "metrics = compute_metrics(labels_arr, labels_pred_arr, seq_lens_arr, vocab_tags)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate Dev##\n",
    "def extract_data(dataset):\n",
    "    \"\"\"Extract words and labels from a dataset.\n",
    "    \n",
    "    Args:\n",
    "      dataset: A CoNLL dataset.\n",
    "    \n",
    "    Returns:\n",
    "      Word ids, char ids, and labels, from a CoNLL dataset,\n",
    "      all as NumPy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    #validation/dev\n",
    "    words_dev, labels_dev = list(minibatches(dev, len(dev)))[0]  \n",
    "    char_ids_dev, word_ids_dev = zip(*words_dev)\n",
    "    word_ids_dev, sequence_lengths_dev = pad_sequences(word_ids_dev, pad_tok=9)\n",
    "    char_ids_dev, word_lengths_dev = pad_sequences(char_ids_dev, pad_tok=9, nlevels=2)\n",
    "    labels_dev, _ = pad_sequences(labels_dev, pad_tok=9)\n",
    "\n",
    "    \n",
    "    word_ids_arr_dev = np.array(word_ids_dev)\n",
    "    char_ids_arr_dev = np.array(char_ids_dev)\n",
    "    labels_arr_dev = np.array(labels_dev)\n",
    "    # TODO: add one-hot encoding of labels\n",
    "    seq_lens_arr_dev = np.array(sequence_lengths_dev)\n",
    "    return word_ids_arr_dev, char_ids_arr_dev, labels_arr_dev, seq_lens_arr_dev\n",
    "\n",
    "\n",
    "def predict_labels(model, word_ids_arr_dev, char_ids_arr_dev, seq_lens_arr_dev, batch_size=32):\n",
    "    \"\"\"Predict labels for a set of words.\n",
    "    \n",
    "    Args:\n",
    "      model: A Keras Model that accepts char ids and word ids\n",
    "        and returns label probs.\n",
    "      word_ids_arr: A NumPy array of word ids for sentences of shape\n",
    "        (num sentences, max num words).\n",
    "      char_ids_arr: A NumPy array of char ids for sentences of shape\n",
    "        (num sentences, max num words, max num chars).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words). \n",
    "    \n",
    "    Returns:\n",
    "      A NumPy array of shape (num sentences, num words)\n",
    "      containing the predicted tags for each word.\n",
    "    \"\"\"\n",
    "#     model.load_weights(\"softmax_with_masking_nine.hdf5\")\n",
    "    labels_prob_arr_dev = model.predict([word_ids_arr_dev, char_ids_arr_dev], batch_size) #shape(num sentences, max num words, num tags)\n",
    "#     labels_prob_arr = model.predict(word_ids_arr, batch_size) #shape(num sentences, max num words, num tags) #DELETE\n",
    "    labels_pred_arr_dev = np.argmax(labels_prob_arr_dev, -1) \n",
    "    return labels_pred_arr_dev\n",
    "\n",
    "\n",
    "def compute_metrics(labels_arr_dev, labels_pred_arr_dev, seq_lens_arr_dev, vocab_tags): #commented out to play with it below but this is the og\n",
    "    \"\"\"Compute accuracy and F1.\n",
    "    \n",
    "    Args:\n",
    "      labels_arr: A NumPy array of correct tags of shape\n",
    "        (num sentences, max num words).\n",
    "      labels_pred_arr: A NumPy array of predicted tags of\n",
    "        shape (num sentences, max num words).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words).\n",
    "      vocab_tags: Dictionary of tag strings to tag numbers.\n",
    "      \n",
    "    Returns:\n",
    "      Dictionary with accuracy `acc` and F1 score `f1`.\n",
    "    \"\"\"\n",
    "    accs_dev = []\n",
    "    correct_preds_dev, total_correct_dev, total_preds_dev = 0., 0., 0.\n",
    "\n",
    "    for lab_dev, lab_pred_dev, seq_len_dev in zip(labels_arr_dev, labels_pred_arr_dev, seq_lens_arr_dev):\n",
    "        # NOTE: labels & predictions are padded to the maximum number of words\n",
    "        # in the batch.  Here, we use the actual sentence lengths to select out\n",
    "        # the actual labels and corresponding predictions.\n",
    "        lab_dev = lab_dev[:seq_len_dev]\n",
    "        lab_pred_dev = lab_pred_dev[:seq_len_dev]\n",
    "        for n, i in enumerate(lab_pred_dev):\n",
    "            if i == 9:\n",
    "                lab_pred_dev[n] = 0\n",
    "        \n",
    "        accs_dev += [a==b for (a, b) in zip(lab_dev, lab_pred_dev)]\n",
    "\n",
    "        lab_chunks_dev = set(get_chunks(lab_dev, vocab_tags))\n",
    "        lab_pred_chunks_dev = set(get_chunks(lab_pred_dev, vocab_tags))\n",
    "\n",
    "        correct_preds_dev += len(lab_chunks_dev & lab_pred_chunks_dev)\n",
    "        total_preds_dev   += len(lab_pred_chunks_dev)\n",
    "        total_correct_dev += len(lab_chunks_dev)\n",
    "        \n",
    "    p_dev   = correct_preds_dev / total_preds_dev if total_preds_dev > 0 else 0 \n",
    "    r_dev   = correct_preds_dev / total_correct_dev if total_correct_dev > 0 else 0\n",
    "    f1_dev  = 2 * p_dev * r_dev / (p_dev + r_dev) if correct_preds_dev > 0 else 0\n",
    "    acc_dev = np.mean(accs_dev)\n",
    "\n",
    "    print ({\"precision\": p_dev})\n",
    "    print ({\"recall\": r_dev})\n",
    "    print ({\"total_correct\": total_correct_dev})\n",
    "    return {\"acc\": 100*acc_dev, \"f1\": 100*f1_dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.7537740760020822}\n",
      "{'recall': 0.7310669808145406}\n",
      "{'total_correct': 5942.0}\n",
      "{'acc': 95.3175499396441, 'f1': 74.2246903032892}\n"
     ]
    }
   ],
   "source": [
    "#dev\n",
    "word_ids_arr_dev, char_ids_arr_dev, labels_arr_dev, seq_lens_arr_dev = extract_data(dev) \n",
    "labels_pred_arr_dev = predict_labels(model_softmax, word_ids_arr_dev, char_ids_arr_dev, seq_lens_arr_dev)\n",
    "metrics = compute_metrics(labels_arr_dev, labels_pred_arr_dev, seq_lens_arr_dev, vocab_tags)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate Test##\n",
    "def extract_data(dataset):\n",
    "    \"\"\"Extract words and labels from a dataset.\n",
    "    \n",
    "    Args:\n",
    "      dataset: A CoNLL dataset.\n",
    "    \n",
    "    Returns:\n",
    "      Word ids, char ids, and labels, from a CoNLL dataset,\n",
    "      all as NumPy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    #test\n",
    "    words_test, labels_test = list(minibatches(test, len(test)))[0]  \n",
    "    char_ids_test, word_ids_test = zip(*words_test)\n",
    "    word_ids_test, sequence_lengths_test = pad_sequences(word_ids_test, pad_tok=9)\n",
    "    char_ids_test, word_lengths_test = pad_sequences(char_ids_test, pad_tok=9, nlevels=2)\n",
    "    labels_test, _ = pad_sequences(labels_test, pad_tok=9)\n",
    "    \n",
    "    word_ids_arr_test = np.array(word_ids_test)\n",
    "    char_ids_arr_test = np.array(char_ids_test)\n",
    "    labels_arr_test = np.array(labels_test)\n",
    "    # TODO: add one-hot encoding of labels\n",
    "    seq_lens_arr_test = np.array(sequence_lengths_test)\n",
    "    return word_ids_arr_test, char_ids_arr_test, labels_arr_test, seq_lens_arr_test\n",
    "\n",
    "\n",
    "def predict_labels(model, word_ids_arr_test, char_ids_arr_test, seq_lens_arr_test, batch_size=32):\n",
    "    \"\"\"Predict labels for a set of words.\n",
    "    \n",
    "    Args:\n",
    "      model: A Keras Model that accepts char ids and word ids\n",
    "        and returns label probs.\n",
    "      word_ids_arr: A NumPy array of word ids for sentences of shape\n",
    "        (num sentences, max num words).\n",
    "      char_ids_arr: A NumPy array of char ids for sentences of shape\n",
    "        (num sentences, max num words, max num chars).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words). \n",
    "    \n",
    "    Returns:\n",
    "      A NumPy array of shape (num sentences, num words)\n",
    "      containing the predicted tags for each word.\n",
    "    \"\"\"\n",
    "#     model.load_weights(\"softmax_with_masking_nine.hdf5\")\n",
    "    labels_prob_arr_test = model.predict([word_ids_arr_test, char_ids_arr_test], batch_size) #shape(num sentences, max num words, num tags)\n",
    "#     labels_prob_arr = model.predict(word_ids_arr, batch_size) #shape(num sentences, max num words, num tags) #DELETE\n",
    "    labels_pred_arr_test = np.argmax(labels_prob_arr_test, -1) \n",
    "    return labels_pred_arr_test\n",
    "\n",
    "\n",
    "def compute_metrics(labels_arr_test, labels_pred_arr_test, seq_lens_arr_test, vocab_tags): #commented out to play with it below but this is the og\n",
    "    \"\"\"Compute accuracy and F1.\n",
    "    \n",
    "    Args:\n",
    "      labels_arr: A NumPy array of correct tags of shape\n",
    "        (num sentences, max num words).\n",
    "      labels_pred_arr: A NumPy array of predicted tags of\n",
    "        shape (num sentences, max num words).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words).\n",
    "      vocab_tags: Dictionary of tag strings to tag numbers.\n",
    "      \n",
    "    Returns:\n",
    "      Dictionary with accuracy `acc` and F1 score `f1`.\n",
    "    \"\"\"\n",
    "    accs_test = []\n",
    "    correct_preds_test, total_correct_test, total_preds_test = 0., 0., 0.\n",
    "\n",
    "    for lab_test, lab_pred_test, seq_len_test in zip(labels_arr_test, labels_pred_arr_test, seq_lens_arr_test):\n",
    "        # NOTE: labels & predictions are padded to the maximum number of words\n",
    "        # in the batch.  Here, we use the actual sentence lengths to select out\n",
    "        # the actual labels and corresponding predictions.\n",
    "        lab_test = lab_test[:seq_len_test]\n",
    "        lab_pred_test = lab_pred_test[:seq_len_test]\n",
    "        for n, i in enumerate(lab_pred_test):\n",
    "            if i == 9:\n",
    "                lab_pred_test[n] = 0\n",
    "        \n",
    "        accs_test += [a==b for (a, b) in zip(lab_test, lab_pred_test)]\n",
    "\n",
    "        lab_chunks_test = set(get_chunks(lab_test, vocab_tags))\n",
    "        lab_pred_chunks_test = set(get_chunks(lab_pred_test, vocab_tags))\n",
    "\n",
    "        correct_preds_test += len(lab_chunks_test & lab_pred_chunks_test)\n",
    "        total_preds_test   += len(lab_pred_chunks_test)\n",
    "        total_correct_test += len(lab_chunks_test)\n",
    "        \n",
    "    p_test   = correct_preds_test / total_preds_test if total_preds_test > 0 else 0 \n",
    "    r_test   = correct_preds_test / total_correct_test if total_correct_test > 0 else 0\n",
    "    f1_test  = 2 * p_test * r_test / (p_test + r_test) if correct_preds_test > 0 else 0\n",
    "    acc_test = np.mean(accs_test)\n",
    "\n",
    "    print ({\"precision\": p_test})\n",
    "    print ({\"recall\": r_test})\n",
    "    print ({\"total_correct\": total_correct_test})\n",
    "    return {\"acc\": 100*acc_test, \"f1\": 100*f1_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.6987951807228916}\n",
      "{'recall': 0.6880311614730878}\n",
      "{'total_correct': 5648.0}\n",
      "{'acc': 94.16819209647895, 'f1': 69.33713979837631}\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "word_ids_arr_test, char_ids_arr_test, labels_arr_test, seq_lens_arr_test = extract_data(test) \n",
    "labels_pred_arr_test = predict_labels(model_softmax, word_ids_arr_test, char_ids_arr_test, seq_lens_arr_test)\n",
    "metrics = compute_metrics(labels_arr_test, labels_pred_arr_test, seq_lens_arr_test, vocab_tags)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 171982,\n",
       " 2: 11101,\n",
       " 3: 7,\n",
       " 4: 3768,\n",
       " 5: 23,\n",
       " 6: 9274,\n",
       " 7: 15,\n",
       " 8: 8184,\n",
       " 9: 1382279}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictor only seems to predict 'O' and 'B-PER' tags\n",
    "# labels_pred_seq_lens_arr = (labels_pred_arr[:seq_lens_arr])\n",
    "unique, counts = np.unique(labels_pred_arr, return_counts=True) #labels are a list not a numpy array\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 43608, 2: 3033, 4: 1007, 6: 1895, 8: 1966, 9: 302741}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_dev, counts_dev = np.unique(labels_pred_arr_dev, return_counts=True) #labels are a list not a numpy array\n",
    "dict(zip(unique_dev, counts_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 39173, 2: 2681, 3: 1, 4: 828, 6: 2345, 8: 1757, 9: 381387}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_test, counts_test = np.unique(labels_pred_arr_test, return_counts=True) #labels are a list not a numpy array\n",
    "dict(zip(unique_test, counts_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

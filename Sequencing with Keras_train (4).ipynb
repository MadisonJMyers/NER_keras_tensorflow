{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmyers/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "# from keras.layers import Flatten, Dense, Embedding, Dropout, Bidirectional, LSTM, Concatenate, Reshape, Lambda, Input, Activation\n",
    "# from keras.layers.merge import concatenate\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.preprocessing.text import one_hot\n",
    "# from keras.utils import np_utils\n",
    "# from keras_contrib.layers import CRF\n",
    "# from model.data_utils import get_trimmed_glove_vectors, load_vocab, get_processing_word, CoNLLDataset, get_trimmed_glove_vectors, load_vocab, get_processing_word, minibatches, get_chunks, pad_sequences\n",
    "# from model.ner_model import NERModel\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Embedding, Dropout, Bidirectional, LSTM, Concatenate, Reshape, Lambda, Input, Activation, Masking\n",
    "from tensorflow.python.keras.layers import concatenate\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import one_hot\n",
    "from keras_contrib.layers import CRF\n",
    "from model.data_utils import get_trimmed_glove_vectors, load_vocab, get_processing_word, CoNLLDataset, get_trimmed_glove_vectors, load_vocab, get_processing_word, minibatches, get_chunks, pad_sequences\n",
    "from model.ner_model import NERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download new data\n",
    "#source: https://github.com/synalp/NER\n",
    "train_filename = \"data/coNLL/eng/eng.train.iob\"\n",
    "dev_filename = \"data/coNLL/eng/eng.testa.iob\"\n",
    "test_filename = \"data/coNLL/eng/eng.testb.iob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_chars = True\n",
    "max_iter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$NUM$': 13,\n",
       " '$UNK$': 9,\n",
       " '.': 16,\n",
       " 'a': 11,\n",
       " 'actor': 2,\n",
       " 'american': 4,\n",
       " 'an': 8,\n",
       " 'and': 0,\n",
       " 'economic': 12,\n",
       " 'european': 22,\n",
       " 'french': 7,\n",
       " 'in': 5,\n",
       " 'is': 1,\n",
       " 'jean': 10,\n",
       " 'lives': 21,\n",
       " 'new': 19,\n",
       " 'oscar': 20,\n",
       " 'pierre': 14,\n",
       " 'political': 17,\n",
       " 'the': 3,\n",
       " 'union': 6,\n",
       " 'won': 18,\n",
       " 'york': 15}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words = load_vocab(\"data/words.txt\")\n",
    "vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 23,\n",
       " 'A': 5,\n",
       " 'E': 17,\n",
       " 'F': 9,\n",
       " 'J': 18,\n",
       " 'N': 8,\n",
       " 'P': 25,\n",
       " 'T': 20,\n",
       " 'U': 21,\n",
       " 'Y': 3,\n",
       " 'a': 15,\n",
       " 'c': 27,\n",
       " 'd': 11,\n",
       " 'e': 7,\n",
       " 'h': 24,\n",
       " 'i': 10,\n",
       " 'k': 16,\n",
       " 'l': 1,\n",
       " 'm': 26,\n",
       " 'n': 12,\n",
       " 'o': 14,\n",
       " 'p': 22,\n",
       " 'r': 4,\n",
       " 's': 13,\n",
       " 't': 6,\n",
       " 'u': 19,\n",
       " 'v': 2,\n",
       " 'w': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_chars = load_vocab(\"data/chars.txt\")\n",
    "vocab_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 7,\n",
       " 'B-MISC': 3,\n",
       " 'B-ORG': 5,\n",
       " 'B-PER': 1,\n",
       " 'I-LOC': 8,\n",
       " 'I-MISC': 4,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 2,\n",
       " 'O': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: this order could be arbitrary, with values in the interval [0, num_tags]\n",
    "# ALSO: there should be a difference between a null tag, and a padded label\n",
    "vocab_tags = load_vocab(\"data/tags.txt\")\n",
    "vocab_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_char = len(vocab_chars)\n",
    "n_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tags = (len(vocab_tags)+1) #+1 if different vocab_tags\n",
    "n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coNLL data for validation\n",
    "dev = CoNLLDataset(dev_filename, get_processing_word(vocab_words, vocab_chars,lowercase=True, chars=use_chars),\n",
    "                  get_processing_word(vocab_tags, lowercase=False, allow_unk=False), max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coNLL data for train\n",
    "train = CoNLLDataset(train_filename, get_processing_word(vocab_words, vocab_chars,lowercase=True, chars=use_chars),\n",
    "                  get_processing_word(vocab_tags, lowercase=False, allow_unk=False), max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coNLL data for test\n",
    "test = CoNLLDataset(test_filename, get_processing_word(vocab_words, vocab_chars,lowercase=True, chars=use_chars),\n",
    "                  get_processing_word(vocab_tags, lowercase=False, allow_unk=False), max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_vocab(filename):\n",
    "    \"\"\"Load vocab from file\n",
    "    Args:\n",
    "        filename: path to the glove vectors\n",
    "    Returns:\n",
    "        vocab: set() of strings\n",
    "    \"\"\"\n",
    "    print(\"Building vocab...\")\n",
    "    vocab = set()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            word = line.strip().split(' ')[0]\n",
    "            vocab.add(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_data = np.load(\"data/glove.6B.300d.trimmed.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = emb_data[\"embeddings\"]\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_word = 300 #End to end paper uses 30\n",
    "dim_char = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_char = 100 # lstm on chars\n",
    "hidden_size_lstm = 300 # lstm on word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.015 #0.001 #End to end uses learning rate of 0.01 for POS tagging and 0.015 for NER where lr is updated on each epoch with decay rate 0.05\n",
    "#End to end also uses gradient clipping of 5.0\n",
    "lr_decay = 0.0005 #GG uses 0.9; paper uses 0.05\n",
    "nepochs = 5 #End to end paper saw best results at 50 epochs\n",
    "batch_size = 10 #20 #End to end paper uses 10 #eval at 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make use of minibatches with fit_generator\n",
    "# for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "words, labels = list(minibatches(train, len(train)))[0]  # NOTE: len(train) will return entire dataset!\n",
    "#GG's version\n",
    "char_ids, word_ids = zip(*words)\n",
    "word_ids, sequence_lengths = pad_sequences(word_ids, pad_tok=9) #word_ids = vocab_chars?\n",
    "char_ids, word_lengths = pad_sequences(char_ids, pad_tok=9, nlevels=2)\n",
    "labels, _ = pad_sequences(labels, pad_tok=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14041, 113)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids_array = np.array(word_ids)\n",
    "word_ids_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation/dev\n",
    "words_dev, labels_dev = list(minibatches(dev, len(dev)))[0]  \n",
    "char_ids_dev, word_ids_dev = zip(*words_dev)\n",
    "word_ids_dev, sequence_lengths_dev = pad_sequences(word_ids_dev, 0)\n",
    "char_ids_dev, word_lengths_dev = pad_sequences(char_ids_dev, pad_tok=0, nlevels=2)\n",
    "labels_dev, _ = pad_sequences(labels_dev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "words_test, labels_test = list(minibatches(test, len(test)))[0]  \n",
    "char_ids_test, word_ids_test = zip(*words_test)\n",
    "word_ids_test, sequence_lengths_test = pad_sequences(word_ids_test, 0)\n",
    "char_ids_test, word_lengths_test = pad_sequences(char_ids_test, pad_tok=0, nlevels=2)\n",
    "labels_test, _ = pad_sequences(labels_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5 # needs to be set before Dropout function- GG 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_input = Input((None,))\n",
    "mask_word = Masking(mask_value=9)(word_emb_input)\n",
    "word_emb_output = Embedding(n_words, dim_word, weights=[embeddings], trainable=False)(mask_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#end to end paper claims to have applied dropout layer on character embeddings before inputting to a CNN in addition to before both layers of BLSTM\n",
    "# char_emb_input = Input((max_seq_length, max_word_length)) \n",
    "char_emb_input = Input((None, None))\n",
    "#comes in as sentences, words, characters and for the character part we want to just operate it over the character sentence by number of words and seq of characters so reshape so we have words by characters\n",
    "char_emb_output = Lambda(lambda x: tf.keras.backend.reshape(x, (-1, tf.keras.backend.shape(x)[-1])))(char_emb_input)\n",
    "mask_char = Masking(mask_value=9)(char_emb_output)  # TODO: make -1 a variable\n",
    "char_emb_output = Embedding(n_char, dim_char)(mask_char) #need weights here?\n",
    "# 2 sided LSTM below that we can change with forward and backward to see which is better performing\n",
    "# char_emb_output = Bidirectional(LSTM(hidden_size_char, return_sequences=False))(char_emb_output)\n",
    "char_emb_output = Dropout(dropout)(char_emb_output)\n",
    "fw_LSTM = LSTM(hidden_size_char, return_sequences=False)(char_emb_output) #is this right?\n",
    "bw_LSTM = LSTM(hidden_size_char, return_sequences=False, go_backwards=True)(char_emb_output)\n",
    "char_emb_output = concatenate([fw_LSTM, bw_LSTM])\n",
    "char_emb_output = Dropout(dropout)(char_emb_output)\n",
    "char_emb_output = Lambda(lambda x, z: tf.keras.backend.reshape(x, (-1, tf.shape(z)[1], 2 * hidden_size_char)), arguments={\"z\": word_emb_input})(char_emb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenates word embedding and character embedding\n",
    "x = concatenate([word_emb_output, char_emb_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(dropout)(x)\n",
    "x = Bidirectional(LSTM(hidden_size_lstm, return_sequences=True))(x)  #should we turn this into two layers (fw and bw)?\n",
    "# fw_LSTM_2 = LSTM(hidden_size_lstm, return_sequences=False)(x) #is this right?\n",
    "# bw_LSTM_2 = LSTM(hidden_size_lstm, return_sequences=False, go_backwards=True)(x)\n",
    "# x = concatenate([fw_LSTM_2, bw_LSTM_2])\n",
    "x = Dropout(dropout)(x)\n",
    "scores = Dense(n_tags)(x) \n",
    "softmax = Activation(\"softmax\")(scores)\n",
    "crf_layer = CRF(n_tags)\n",
    "# crf = crf_layer(scores) #should we add this to attach to the softmax model? with SGD and gradiet clipping of 5.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_softmax = Model([word_emb_input, char_emb_input], softmax) #should these be input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf = Model([word_emb_input, char_emb_input], crf) #should these be input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None)         0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    2800        masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 100)    0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100)          80400       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100)          80400       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    6900        masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 200)    0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 500)    0           embedding_1[0][0]                \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 500)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 600)    1922400     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, 600)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 10)     6010        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 10)     0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,098,910\n",
      "Trainable params: 2,092,010\n",
      "Non-trainable params: 6,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_softmax.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_softmax.predict([np.random.randn(10, 100), np.random.randn(10, 100, 12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_op = Adam(lr=lr, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_softmax.compile(loss=\"categorical_crossentropy\", optimizer=adam_op, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.compile(loss=crf_layer.loss_function, optimizer=adam_op, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "char_ids_arr = np.array(char_ids)\n",
    "word_ids_arr = np.array(word_ids)\n",
    "labels_arr = np.array(labels)\n",
    "labels_arr_one_hot = np.eye(10)[labels] #10 if vocab_tags are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev/validation\n",
    "char_ids_arr_dev = np.array(char_ids_dev)\n",
    "word_ids_arr_dev = np.array(word_ids_dev)\n",
    "labels_arr_dev = np.array(labels_dev)\n",
    "labels_arr_one_hot_dev = np.eye(10)[labels_dev] #10 if vocab_tags are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "char_ids_arr_test = np.array(char_ids_test)\n",
    "word_ids_arr_test = np.array(word_ids_test)\n",
    "labels_arr_test = np.array(labels_test)\n",
    "labels_arr_one_hot_test = np.eye(10)[labels_test] #10 if vocab_tags are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = datetime.strftime(datetime.today(), \"%y%m%d_%H%M%S\")\n",
    "# base_dir = f\"models/{date}\"\n",
    "# if not os.path.exists(base_dir):\n",
    "#     os.makedirs(base_dir)\n",
    "# model_checkpoint = keras.callbacks.ModelCheckpoint(base_dir + \"/{val_loss}_{epoch:03d}.hdf5\")\n",
    "# tb_callback = keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "# callbacks = [model_checkpoint, tb_callback]\n",
    "# print(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14041, 113)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14041, 113, 24)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ids_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14041, 113, 10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_arr_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14041 samples, validate on 3250 samples\n",
      "Epoch 1/5\n",
      "14041/14041 [==============================] - 837s 60ms/step - loss: 0.1008 - acc: 0.9705 - val_loss: 7.9200 - val_acc: 0.0462\n",
      "Epoch 2/5\n",
      "14041/14041 [==============================] - 834s 59ms/step - loss: 0.0699 - acc: 0.9780 - val_loss: 8.4597 - val_acc: 0.0453\n",
      "Epoch 3/5\n",
      "14041/14041 [==============================] - 834s 59ms/step - loss: 0.0650 - acc: 0.9794 - val_loss: 9.5366 - val_acc: 0.0418\n",
      "Epoch 4/5\n",
      "14041/14041 [==============================] - 832s 59ms/step - loss: 0.0611 - acc: 0.9806 - val_loss: 8.4824 - val_acc: 0.0332\n",
      "Epoch 5/5\n",
      "14041/14041 [==============================] - 832s 59ms/step - loss: 0.0582 - acc: 0.9814 - val_loss: 10.5266 - val_acc: 0.0478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7faf40131048>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add callbacks:\n",
    "# early stopping and saving best parameters\n",
    "# learning rate decay\n",
    "# tensorboard\n",
    "# number of epochs without improving is 0 (for early stopping)\n",
    "# could add gradient clipping (optional)\n",
    "model_softmax.fit([word_ids_arr, char_ids_arr], labels_arr_one_hot, batch_size=batch_size, epochs=nepochs, validation_data=([word_ids_arr_dev, char_ids_arr_dev], labels_arr_one_hot_dev)) \n",
    "#fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "#fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "\n",
    "# model_softmax.save(f\"{base_dir}/train_softmax.hdf5\") #final_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_softmax.save_weights(\"softmax_test_5_14.hdf5\")\n",
    "# model_softmax.save_weights(\"softmax_test_5_16.hdf5\")\n",
    "model_softmax.save_weights(\"softmax_test_5_17_1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.fit([word_ids_arr, char_ids_arr], labels_arr_one_hot, batch_size=batch_size, epochs=nepochs, validation_split=0.33)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_crf.save(\"crf.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.save(\"crf_with_val.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir  #models/180222_215523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lolz why so slow?\n",
    "# model_softmax.load_weights(\"softmax_test2.hdf5\") #100 epochs but symptoms of exploding gradients leading me to wanting to add gradient clipping\n",
    "# model_softmax.load_weights(\"softmax_test.hdf5\") #with 75 epochs on same tuning as test2\n",
    "# model_softmax.load_weights(\"softmax_test_5_14.hdf5\") #with 75 epochs on same tuning as test2 different day\n",
    "model_softmax.load_weights(\"softmax_test_5_17_1.hdf5\")\n",
    "# model_softmax.load_weights(\"softmax_with_masking_nine.hdf5\")\n",
    "# model_softmax.load_weights(\"softmax_with_masking_neg1.hdf5\")\n",
    "# model_softmax.load_weights(f\"{base_dir}/train_softmax.hdf5\")#\"models/180222_215523/final_softmax.hdf5\")#\"0.11342436582348703_050.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_softmax = model_softmax.predict([word_ids_arr, char_ids_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_crf.load_weights(\"crf_with_val.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_crf = model_crf.predict([word_ids_arr, char_ids_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prediction_crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "# scores_crf = model_crf.evaluate([word_ids_arr, char_ids_arr], labels_arr_one_hot) #x_test, y_test (when testing)\n",
    "# print(\"%s: %.2f%%\" % (model_crf.metrics_names[1], scores_crf[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_softmax = model_softmax.evaluate([word_ids_arr, char_ids_arr], labels_arr_one_hot) #x_test, y_test (when testing)\n",
    "# print(\"%s: %.2f%%\" % (model_softmax.metrics_names[1], scores_softmax[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train F1 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate Training##\n",
    "def extract_data(dataset):\n",
    "    \"\"\"Extract words and labels from a dataset.\n",
    "    \n",
    "    Args:\n",
    "      dataset: A CoNLL dataset.\n",
    "    \n",
    "    Returns:\n",
    "      Word ids, char ids, and labels, from a CoNLL dataset,\n",
    "      all as NumPy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    words, labels = list(minibatches(dataset, len(dataset)))[0]  # NOTE: len(train) will return entire dataset!\n",
    "    char_ids, word_ids = zip(*words)\n",
    "    \n",
    "    word_ids, sequence_lengths = pad_sequences(word_ids, pad_tok=9)\n",
    "    char_ids, word_lengths = pad_sequences(char_ids, pad_tok=9, nlevels=2)\n",
    "    labels, _ = pad_sequences(labels, pad_tok=9)\n",
    "\n",
    "    word_ids_arr = np.array(word_ids)\n",
    "    char_ids_arr = np.array(char_ids)\n",
    "    labels_arr = np.array(labels)\n",
    "    # TODO: add one-hot encoding of labels\n",
    "    seq_lens_arr = np.array(sequence_lengths)\n",
    "    return word_ids_arr, char_ids_arr, labels_arr, seq_lens_arr\n",
    "\n",
    "\n",
    "def predict_labels(model, word_ids_arr, char_ids_arr, seq_lens_arr, batch_size=32):\n",
    "    \"\"\"Predict labels for a set of words.\n",
    "    \n",
    "    Args:\n",
    "      model: A Keras Model that accepts char ids and word ids\n",
    "        and returns label probs.\n",
    "      word_ids_arr: A NumPy array of word ids for sentences of shape\n",
    "        (num sentences, max num words).\n",
    "      char_ids_arr: A NumPy array of char ids for sentences of shape\n",
    "        (num sentences, max num words, max num chars).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words). \n",
    "    \n",
    "    Returns:\n",
    "      A NumPy array of shape (num sentences, num words)\n",
    "      containing the predicted tags for each word.\n",
    "    \"\"\"\n",
    "#     model.load_weights(\"softmax_with_masking_nine.hdf5\")\n",
    "    labels_prob_arr = model.predict([word_ids_arr, char_ids_arr], batch_size) #shape(num sentences, max num words, num tags)\n",
    "#     labels_prob_arr = model.predict(word_ids_arr, batch_size) #shape(num sentences, max num words, num tags) #DELETE\n",
    "    labels_pred_arr = np.argmax(labels_prob_arr, -1) \n",
    "    return labels_pred_arr\n",
    "\n",
    "\n",
    "def compute_metrics(labels_arr, labels_pred_arr, seq_lens_arr, vocab_tags): #commented out to play with it below but this is the og\n",
    "    \"\"\"Compute accuracy and F1.\n",
    "    \n",
    "    Args:\n",
    "      labels_arr: A NumPy array of correct tags of shape\n",
    "        (num sentences, max num words).\n",
    "      labels_pred_arr: A NumPy array of predicted tags of\n",
    "        shape (num sentences, max num words).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words).\n",
    "      vocab_tags: Dictionary of tag strings to tag numbers.\n",
    "      \n",
    "    Returns:\n",
    "      Dictionary with accuracy `acc` and F1 score `f1`.\n",
    "    \"\"\"\n",
    "    accs = []\n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "\n",
    "    for lab, lab_pred, seq_len in zip(labels_arr, labels_pred_arr, seq_lens_arr):\n",
    "        # NOTE: labels & predictions are padded to the maximum number of words\n",
    "        # in the batch.  Here, we use the actual sentence lengths to select out\n",
    "        # the actual labels and corresponding predictions.\n",
    "        lab = lab[:seq_len]\n",
    "        lab_pred = lab_pred[:seq_len]\n",
    "        for n, i in enumerate(lab_pred):\n",
    "            if i == 9:\n",
    "                lab_pred[n] = 0\n",
    "        \n",
    "        accs += [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "        \n",
    "        lab_chunks      = set(get_chunks(lab, vocab_tags))\n",
    "        lab_pred_chunks = set(get_chunks(lab_pred, vocab_tags))\n",
    "\n",
    "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "        total_preds   += len(lab_pred_chunks)\n",
    "        total_correct += len(lab_chunks)\n",
    "\n",
    "        \n",
    "    p   = correct_preds / total_preds if total_preds > 0 else 0 \n",
    "    r   = correct_preds / total_correct if total_correct > 0 else 0\n",
    "    f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "    acc = np.mean(accs)\n",
    "\n",
    "    print ({\"precision\": p})\n",
    "    print ({\"recall\": r})\n",
    "    print ({\"total_correct\": total_correct})\n",
    "    return {\"acc\": 100*acc, \"f1\": 100*f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.5564379858060156}\n",
      "{'recall': 0.42040086812204774}\n",
      "{'total_correct': 23499.0}\n",
      "{'acc': 90.10956630210048, 'f1': 47.89469856737692}\n"
     ]
    }
   ],
   "source": [
    "word_ids_arr, char_ids_arr, labels_arr, seq_lens_arr = extract_data(train) \n",
    "labels_pred_arr = predict_labels(model_softmax, word_ids_arr, char_ids_arr, seq_lens_arr)\n",
    "metrics = compute_metrics(labels_arr, labels_pred_arr, seq_lens_arr, vocab_tags)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate Dev##\n",
    "def extract_data(dataset):\n",
    "    \"\"\"Extract words and labels from a dataset.\n",
    "    \n",
    "    Args:\n",
    "      dataset: A CoNLL dataset.\n",
    "    \n",
    "    Returns:\n",
    "      Word ids, char ids, and labels, from a CoNLL dataset,\n",
    "      all as NumPy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    words_dev, labels_dev = list(minibatches(dataset, len(dataset)))[0]  \n",
    "    char_ids_dev, word_ids_dev = zip(*words_dev)\n",
    "    word_ids_dev, sequence_lengths_dev = pad_sequences(word_ids_dev, 0)\n",
    "    char_ids_dev, word_lengths_dev = pad_sequences(char_ids_dev, pad_tok=0, nlevels=2)\n",
    "    labels_dev, _ = pad_sequences(labels_dev, 0)\n",
    "    \n",
    "    word_ids_arr_dev = np.array(word_ids_dev)\n",
    "    char_ids_arr_dev = np.array(char_ids_dev)\n",
    "    labels_arr_dev = np.array(labels_dev)\n",
    "    # TODO: add one-hot encoding of labels\n",
    "    seq_lens_arr_dev = np.array(sequence_lengths_dev)\n",
    "    return word_ids_arr_dev, char_ids_arr_dev, labels_arr_dev, seq_lens_arr_dev\n",
    "\n",
    "\n",
    "def predict_labels(model, word_ids_arr_dev, char_ids_arr_dev, seq_lens_arr_dev, batch_size=32):\n",
    "    \"\"\"Predict labels for a set of words.\n",
    "    \n",
    "    Args:\n",
    "      model: A Keras Model that accepts char ids and word ids\n",
    "        and returns label probs.\n",
    "      word_ids_arr: A NumPy array of word ids for sentences of shape\n",
    "        (num sentences, max num words).\n",
    "      char_ids_arr: A NumPy array of char ids for sentences of shape\n",
    "        (num sentences, max num words, max num chars).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words). \n",
    "    \n",
    "    Returns:\n",
    "      A NumPy array of shape (num sentences, num words)\n",
    "      containing the predicted tags for each word.\n",
    "    \"\"\"\n",
    "#     model.load_weights(\"softmax_with_masking_nine.hdf5\")\n",
    "    labels_prob_arr_dev = model.predict([word_ids_arr_dev, char_ids_arr_dev], batch_size) #shape(num sentences, max num words, num tags)\n",
    "#     labels_prob_arr = model.predict(word_ids_arr, batch_size) #shape(num sentences, max num words, num tags) #DELETE\n",
    "    labels_pred_arr_dev = np.argmax(labels_prob_arr_dev, -1) \n",
    "    return labels_pred_arr_dev\n",
    "\n",
    "\n",
    "def compute_metrics(labels_arr_dev, labels_pred_arr_dev, seq_lens_arr_dev, vocab_tags): #commented out to play with it below but this is the og\n",
    "    \"\"\"Compute accuracy and F1.\n",
    "    \n",
    "    Args:\n",
    "      labels_arr: A NumPy array of correct tags of shape\n",
    "        (num sentences, max num words).\n",
    "      labels_pred_arr: A NumPy array of predicted tags of\n",
    "        shape (num sentences, max num words).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words).\n",
    "      vocab_tags: Dictionary of tag strings to tag numbers.\n",
    "      \n",
    "    Returns:\n",
    "      Dictionary with accuracy `acc` and F1 score `f1`.\n",
    "    \"\"\"\n",
    "    accs_dev = []\n",
    "    correct_preds_dev, total_correct_dev, total_preds_dev = 0., 0., 0.\n",
    "\n",
    "    for lab_dev, lab_pred_dev, seq_len_dev in zip(labels_arr_dev, labels_pred_arr_dev, seq_lens_arr_dev):\n",
    "        # NOTE: labels & predictions are padded to the maximum number of words\n",
    "        # in the batch.  Here, we use the actual sentence lengths to select out\n",
    "        # the actual labels and corresponding predictions.\n",
    "        lab_dev = lab_dev[:seq_len_dev]\n",
    "        lab_pred_dev = lab_pred_dev[:seq_len_dev]\n",
    "        for n, i in enumerate(lab_pred_dev):\n",
    "            if i == 9:\n",
    "                lab_pred_dev[n] = 0\n",
    "        \n",
    "        accs_dev += [a==b for (a, b) in zip(lab_dev, lab_pred_dev)]\n",
    "\n",
    "        lab_chunks_dev = set(get_chunks(lab_dev, vocab_tags))\n",
    "        lab_pred_chunks_dev = set(get_chunks(lab_pred_dev, vocab_tags))\n",
    "\n",
    "        correct_preds_dev += len(lab_chunks_dev & lab_pred_chunks_dev)\n",
    "        total_preds_dev   += len(lab_pred_chunks_dev)\n",
    "        total_correct_dev += len(lab_chunks_dev)\n",
    "        \n",
    "    p_dev   = correct_preds_dev / total_preds_dev if total_preds_dev > 0 else 0 \n",
    "    r_dev   = correct_preds_dev / total_correct_dev if total_correct_dev > 0 else 0\n",
    "    f1_dev  = 2 * p_dev * r_dev / (p_dev + r_dev) if correct_preds_dev > 0 else 0\n",
    "    acc_dev = np.mean(accs_dev)\n",
    "\n",
    "    print ({\"precision\": p_dev})\n",
    "    print ({\"recall\": r_dev})\n",
    "    print ({\"total_correct\": total_correct_dev})\n",
    "    return {\"acc\": 100*acc_dev, \"f1\": 100*f1_dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.18867924528301888}\n",
      "{'recall': 0.001682935038707506}\n",
      "{'total_correct': 5942.0}\n",
      "{'acc': 83.3281414275145, 'f1': 0.3336113427856547}\n"
     ]
    }
   ],
   "source": [
    "#dev\n",
    "word_ids_arr_dev, char_ids_arr_dev, labels_arr_dev, seq_lens_arr_dev = extract_data(dev) \n",
    "labels_pred_arr_dev = predict_labels(model_softmax, word_ids_arr_dev, char_ids_arr_dev, seq_lens_arr_dev)\n",
    "metrics = compute_metrics(labels_arr_dev, labels_pred_arr_dev, seq_lens_arr_dev, vocab_tags)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate Test##\n",
    "def extract_data(dataset):\n",
    "    \"\"\"Extract words and labels from a dataset.\n",
    "    \n",
    "    Args:\n",
    "      dataset: A CoNLL dataset.\n",
    "    \n",
    "    Returns:\n",
    "      Word ids, char ids, and labels, from a CoNLL dataset,\n",
    "      all as NumPy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    words_test, labels_test = list(minibatches(dataset, len(dataset)))[0]  \n",
    "    char_ids_test, word_ids_test = zip(*words_test)\n",
    "    word_ids_test, sequence_lengths_test = pad_sequences(word_ids_test, 0)\n",
    "    char_ids_test, word_lengths_test = pad_sequences(char_ids_test, pad_tok=0, nlevels=2)\n",
    "    labels_test, _ = pad_sequences(labels_test, 0)\n",
    "    \n",
    "    word_ids_arr_test = np.array(word_ids_test)\n",
    "    char_ids_arr_test = np.array(char_ids_test)\n",
    "    labels_arr_test = np.array(labels_test)\n",
    "    # TODO: add one-hot encoding of labels\n",
    "    seq_lens_arr_test = np.array(sequence_lengths_test)\n",
    "    return word_ids_arr_test, char_ids_arr_test, labels_arr_test, seq_lens_arr_test\n",
    "\n",
    "\n",
    "def predict_labels(model, word_ids_arr_test, char_ids_arr_test, seq_lens_arr_test, batch_size=32):\n",
    "    \"\"\"Predict labels for a set of words.\n",
    "    \n",
    "    Args:\n",
    "      model: A Keras Model that accepts char ids and word ids\n",
    "        and returns label probs.\n",
    "      word_ids_arr: A NumPy array of word ids for sentences of shape\n",
    "        (num sentences, max num words).\n",
    "      char_ids_arr: A NumPy array of char ids for sentences of shape\n",
    "        (num sentences, max num words, max num chars).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words). \n",
    "    \n",
    "    Returns:\n",
    "      A NumPy array of shape (num sentences, num words)\n",
    "      containing the predicted tags for each word.\n",
    "    \"\"\"\n",
    "#     model.load_weights(\"softmax_with_masking_nine.hdf5\")\n",
    "    labels_prob_arr_test = model.predict([word_ids_arr_test, char_ids_arr_test], batch_size) #shape(num sentences, max num words, num tags)\n",
    "#     labels_prob_arr = model.predict(word_ids_arr, batch_size) #shape(num sentences, max num words, num tags) #DELETE\n",
    "    labels_pred_arr_test = np.argmax(labels_prob_arr_test, -1) \n",
    "    return labels_pred_arr_test\n",
    "\n",
    "\n",
    "def compute_metrics(labels_arr_test, labels_pred_arr_test, seq_lens_arr_test, vocab_tags): #commented out to play with it below but this is the og\n",
    "    \"\"\"Compute accuracy and F1.\n",
    "    \n",
    "    Args:\n",
    "      labels_arr: A NumPy array of correct tags of shape\n",
    "        (num sentences, max num words).\n",
    "      labels_pred_arr: A NumPy array of predicted tags of\n",
    "        shape (num sentences, max num words).\n",
    "      seq_lens_arr: A NumPy array of sentence lengths, of\n",
    "        shape (num sentences, actual num words).\n",
    "      vocab_tags: Dictionary of tag strings to tag numbers.\n",
    "      \n",
    "    Returns:\n",
    "      Dictionary with accuracy `acc` and F1 score `f1`.\n",
    "    \"\"\"\n",
    "    accs_test = []\n",
    "    correct_preds_test, total_correct_test, total_preds_test = 0., 0., 0.\n",
    "\n",
    "    for lab_test, lab_pred_test, seq_len_test in zip(labels_arr_test, labels_pred_arr_test, seq_lens_arr_test):\n",
    "        # NOTE: labels & predictions are padded to the maximum number of words\n",
    "        # in the batch.  Here, we use the actual sentence lengths to select out\n",
    "        # the actual labels and corresponding predictions.\n",
    "        lab_test = lab_test[:seq_len_test]\n",
    "        lab_pred_test = lab_pred_test[:seq_len_test]\n",
    "        for n, i in enumerate(lab_pred_test):\n",
    "            if i == 9:\n",
    "                lab_pred_test[n] = 0\n",
    "        \n",
    "        accs_test += [a==b for (a, b) in zip(lab_test, lab_pred_test)]\n",
    "\n",
    "        lab_chunks_test = set(get_chunks(lab_test, vocab_tags))\n",
    "        lab_pred_chunks_test = set(get_chunks(lab_pred_test, vocab_tags))\n",
    "\n",
    "        correct_preds_test += len(lab_chunks_test & lab_pred_chunks_test)\n",
    "        total_preds_test   += len(lab_pred_chunks_test)\n",
    "        total_correct_test += len(lab_chunks_test)\n",
    "        \n",
    "    p_test   = correct_preds_test / total_preds_test if total_preds_test > 0 else 0 \n",
    "    r_test   = correct_preds_test / total_correct_test if total_correct_test > 0 else 0\n",
    "    f1_test  = 2 * p_test * r_test / (p_test + r_test) if correct_preds_test > 0 else 0\n",
    "    acc_test = np.mean(accs_test)\n",
    "\n",
    "    print ({\"precision\": p_test})\n",
    "    print ({\"recall\": r_test})\n",
    "    print ({\"total_correct\": total_correct_test})\n",
    "    return {\"acc\": 100*acc_test, \"f1\": 100*f1_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.25}\n",
      "{'recall': 0.0030099150141643057}\n",
      "{'total_correct': 5648.0}\n",
      "{'acc': 82.62086787983203, 'f1': 0.5948215535339397}\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "word_ids_arr_test, char_ids_arr_test, labels_arr_test, seq_lens_arr_test = extract_data(test) \n",
    "labels_pred_arr_test = predict_labels(model_softmax, word_ids_arr_test, char_ids_arr_test, seq_lens_arr_test)\n",
    "metrics = compute_metrics(labels_arr_test, labels_pred_arr_test, seq_lens_arr_test, vocab_tags)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 181829, 2: 9219, 4: 1047, 6: 5524, 8: 7182, 9: 1381832}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictor only seems to predict 'O' and 'B-PER' tags\n",
    "# labels_pred_seq_lens_arr = (labels_pred_arr[:seq_lens_arr])\n",
    "unique, counts = np.unique(labels_pred_arr, return_counts=True) #labels are a list not a numpy array\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 51609, 2: 1, 4: 18, 6: 2, 8: 36, 9: 302584}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_dev, counts_dev = np.unique(labels_pred_arr_dev, return_counts=True) #labels are a list not a numpy array\n",
    "dict(zip(unique_dev, counts_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 46693, 4: 20, 6: 1, 8: 57, 9: 381401}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_test, counts_test = np.unique(labels_pred_arr_test, return_counts=True) #labels are a list not a numpy array\n",
    "dict(zip(unique_test, counts_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
